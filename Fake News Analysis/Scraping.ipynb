{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b800004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\brian\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\brian\\anaconda3\\lib\\site-packages (2.32.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: pandas in c:\\users\\brian\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\brian\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\brian\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: outcome in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\brian\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\brian\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\brian\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\brian\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\brian\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brian\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install lxml\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351f68d",
   "metadata": {},
   "source": [
    "## Scrape Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40c6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Scraping Breitbart\n",
    "\n",
    "fname = 'Breitbart'\n",
    "\n",
    "output = {'title':[],'text':[]}\n",
    "\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options, service=Service(ChromeDriverManager().install())\n",
    ")\n",
    "\n",
    "url = \"https://www.breitbart.com/sports/2024/11/06/dont-let-the-door-hit-you-mlb-great-roger-clemens-taunts-celebs-who-promised-to-leave-if-trump-won/\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "# Get data\n",
    "text = soup.find('div', class_='entry-content').get_text()\n",
    "title = soup.find('h1').get_text()\n",
    "\n",
    "\n",
    "# Append\n",
    "output['title'].append(title)\n",
    "output['text'].append(text)\n",
    "\n",
    "# Save\n",
    "results = pandas.DataFrame(output)\n",
    "results.to_csv(f'{fname}.csv', index=True, index_label=\"Index\",encoding=\"utf-8-sig\")\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16d4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Scraping Breitbart\n",
    "\n",
    "fname = 'Breitbart2'\n",
    "\n",
    "output = {'title':[],'text':[]}\n",
    "\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options, service=Service(ChromeDriverManager().install())\n",
    ")\n",
    "\n",
    "url = \"https://www.breitbart.com/2024-election/2024/11/05/donald-trump-wins-the-presidency-greatest-comeback-in-american-history/\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "# Get data\n",
    "text = soup.find('div', class_='entry-content').get_text()\n",
    "title = soup.find('h1').get_text()\n",
    "\n",
    "\n",
    "# Append\n",
    "output['title'].append(title)\n",
    "output['text'].append(text)\n",
    "\n",
    "# Save\n",
    "results = pandas.DataFrame(output)\n",
    "results.to_csv(f'{fname}.csv', index=True, index_label=\"Index\",encoding=\"utf-8-sig\")\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212be9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Scraping Breitbart\n",
    "\n",
    "fname = 'InfoWars'\n",
    "\n",
    "output = {'title':[],'text':[]}\n",
    "\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options, service=Service(ChromeDriverManager().install())\n",
    ")\n",
    "\n",
    "url = \"https://www.infowars.com/posts/watch-msnbcs-chris-hayes-calls-on-deep-state-to-hinder-trump-agenda\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "# Get data\n",
    "text = soup.find('div', class_='article-details').get_text()\n",
    "title = soup.find('h1').get_text()\n",
    "\n",
    "\n",
    "# Append\n",
    "output['title'].append(title)\n",
    "output['text'].append(text)\n",
    "\n",
    "# Save\n",
    "results = pandas.DataFrame(output)\n",
    "results.to_csv(f'{fname}.csv', index=True, index_label=\"Index\",encoding=\"utf-8-sig\")\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fde9f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Scraping Breitbart\n",
    "\n",
    "fname = 'InfoWars2'\n",
    "\n",
    "output = {'title':[],'text':[]}\n",
    "\n",
    "\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options, service=Service(ChromeDriverManager().install())\n",
    ")\n",
    "\n",
    "url = \"https://www.infowars.com/posts/harris-officially-concedes-stresses-peaceful-transfer-of-power\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "# Get data\n",
    "text = soup.find('div', class_='article-details').get_text()\n",
    "title = soup.find('h1').get_text()\n",
    "\n",
    "\n",
    "# Append\n",
    "output['title'].append(title)\n",
    "output['text'].append(text)\n",
    "\n",
    "# Save\n",
    "results = pandas.DataFrame(output)\n",
    "results.to_csv(f'{fname}.csv', index=True, index_label=\"Index\",encoding=\"utf-8-sig\")\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac00141",
   "metadata": {},
   "source": [
    "## Scrape Real News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b343be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Save Vox Articles to CSV\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "fname='Vox'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "url = 'https://www.vox.com/2024-elections/383197/kamala-harris-results-underperformed-democratic-senate-candidates'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract Title \n",
    "title_tag = soup.select_one('title')  # Selects the first <title> element\n",
    "\n",
    "# Extract Author\n",
    "author = soup.find('span', class_=lambda x: x and '15r56j12' in x).get_text()\n",
    "\n",
    "# Extract Date (Timestamp)\n",
    "timestamp = soup.find('span', class_='duet--article--timestamp').get_text()\n",
    "\n",
    "# Find the first img tag in the document\n",
    "img_tag = soup.find('img')\n",
    "\n",
    "# Extract Image Url\n",
    "if img_tag:\n",
    "    image_url = img_tag.get('srcset').split()[0]\n",
    "\n",
    "# Extract photo caption\n",
    "figcaption = soup.find('figcaption').get_text()\n",
    "\n",
    "# Extract article text\n",
    "article_text = \"\"\n",
    "article_body = soup.find_all('div', class_='duet--article--article-body-component')\n",
    "\n",
    "if not article_body:\n",
    "    article_text += \"Article body not found with the specified class. Trying a different approach.\\n\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for para in paragraphs:\n",
    "        article_text += para.get_text().strip() + \"\\n\"\n",
    "else:\n",
    "    for div in article_body:\n",
    "        paragraphs = div.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            article_text += paragraph.get_text().strip() + \"\\n\"\n",
    "\n",
    "# Create a CSV file and write the data\n",
    "with open(f'{fname}.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Title', 'Author', 'Timestamp', 'Image URL', 'Figcaption', 'Article Text'])\n",
    "    \n",
    "    # Write the article data\n",
    "    writer.writerow([title_tag, author, timestamp, image_url, figcaption, article_text])\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88336787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Save Vox Articles to CSV\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "fname='Vox2'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "url = 'https://www.vox.com/policy/383286/chart-trump-immigration-crackdown-private-prison'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract Title \n",
    "title_tag = soup.select_one('title')  # Selects the first <title> element\n",
    "\n",
    "# Extract Author\n",
    "author = soup.find('span', class_=lambda x: x and '15r56j12' in x).get_text()\n",
    "\n",
    "# Extract Date (Timestamp)\n",
    "timestamp = soup.find('span', class_='duet--article--timestamp').get_text()\n",
    "\n",
    "# Find the first img tag in the document\n",
    "img_tag = soup.find('img')\n",
    "\n",
    "# Extract Image Url\n",
    "if img_tag:\n",
    "    image_url = img_tag.get('srcset').split()[0]\n",
    "\n",
    "# Extract photo caption\n",
    "figcaption = soup.find('figcaption').get_text()\n",
    "\n",
    "# Extract article text\n",
    "article_text = \"\"\n",
    "article_body = soup.find_all('div', class_='duet--article--article-body-component')\n",
    "\n",
    "if not article_body:\n",
    "    article_text += \"Article body not found with the specified class. Trying a different approach.\\n\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for para in paragraphs:\n",
    "        article_text += para.get_text().strip() + \"\\n\"\n",
    "else:\n",
    "    for div in article_body:\n",
    "        paragraphs = div.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            article_text += paragraph.get_text().strip() + \"\\n\"\n",
    "\n",
    "# Create a CSV file and write the data\n",
    "with open(f'{fname}.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Title', 'Author', 'Timestamp', 'Image URL', 'Figcaption', 'Article Text'])\n",
    "    \n",
    "    # Write the article data\n",
    "    writer.writerow([title_tag, author, timestamp, image_url, figcaption, article_text])\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacbb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime: 2024-11-06\n",
      "Displayed Date: November 06, 2024\n",
      "Caption: U.S. Vice President Kamala Harris speaks at a campaign rally on Nov. 4, 2024, in Allentown, Pennsylvania. (Michael M. Santiago/Getty Images)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Save Daily Signal Articles to CSV\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "fname = 'DailySignal'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "}\n",
    "url = 'https://www.dailysignal.com/2024/11/06/james-carville-breaks-down-big-mistakes-that-doomed-harris-presidential-bid/'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the <h1> tag with id 'post-title' and 'entry-title'\n",
    "title_tag = soup.find('h1', {'id': 'post-title entry-title'})\n",
    "\n",
    "\n",
    "# Author\n",
    "all_author_tags = soup.find_all('a')\n",
    "\n",
    "# Filter the tags to find the author's link\n",
    "author_names = []\n",
    "for tag in all_author_tags:\n",
    "    # Check if the href contains 'author'\n",
    "    if 'author' in tag.get('href', ''):\n",
    "        # Append the author's name to the list, stripping any extra whitespace\n",
    "        author_names.append(tag.get_text(strip=True))\n",
    "\n",
    "\n",
    "author_name = author_names[0] # Author name is the first <a> tag \n",
    "\n",
    "\n",
    "# Date\n",
    "time_tag = soup.find('time', class_='date')\n",
    "\n",
    "# If the time tag is found, extract the datetime attribute and/or the text content\n",
    "if time_tag:\n",
    "    date_time = time_tag.get('datetime')  # Get the datetime attribute\n",
    "    display_date = time_tag.get_text(strip=True)  # Get the displayed date\n",
    "    print(f\"Datetime: {date_time}\")\n",
    "    print(f\"Displayed Date: {display_date}\")\n",
    "else:\n",
    "    print(\"Date not found\")\n",
    "\n",
    "\n",
    "\n",
    "# Find the first <img> tag  \n",
    "img_tag = soup.find('img')\n",
    "\n",
    "img_link = img_tag['src']\n",
    "\n",
    "\n",
    "#  Get Caption \n",
    "caption_tag = soup.find('p', class_='caption')\n",
    "\n",
    "if caption_tag:\n",
    "    caption_text = caption_tag.get_text(strip=True)  # Get the caption text and strip whitespace\n",
    "    print(f\"Caption: {caption_text}\")\n",
    "else:\n",
    "    print(\"Caption not found\")\n",
    "\n",
    "\n",
    "# Get TEXT \n",
    "content_div = soup.find('div', class_='tds-content')\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "# If the content div is found, find all <p> tags within it\n",
    "if content_div:\n",
    "    paragraphs = content_div.find_all('p')  # Find all <p> tags\n",
    "    for tag in paragraphs:\n",
    "        text += tag.get_text()\n",
    "\n",
    "    # Extract and print the text from each <p> tag\n",
    "    #for i, p in enumerate(paragraphs, start=1):\n",
    "    #    print(f\"Paragraph {i}: {p.get_text(strip=True)}\")  # Print each paragraph with a number\n",
    "else:\n",
    "    print(\"Content div not found\")\n",
    "\n",
    "cleaned_text = re.sub(r'\\bpic\\.twitter\\S*', '', text)\n",
    "\n",
    "    \n",
    "    \n",
    "# Create a CSV file and write the data\n",
    "with open(f'{fname}.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Title', 'Author', 'Date', 'Image URL', 'Figcaption', 'Article Text'])\n",
    "    \n",
    "    # Write the article data\n",
    "    writer.writerow([title_tag.get_text(), author_name, display_date, img_link, caption_text, cleaned_text])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c634a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime: 2024-11-06\n",
      "Displayed Date: November 06, 2024\n",
      "Caption: CBS News anchors Norah O’Donnell, left, and Margaret Brennan moderate the Oct. 1 vice-presidential debate. (Anna Moneymaker/Getty Images)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Save Daily Signal Articles to CSV\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "fname = 'DailySignal2'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "}\n",
    "url = 'https://www.dailysignal.com/2024/11/06/corporate-media-is-this-elections-big-loser/'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content of the response\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the <h1> tag with id 'post-title' and 'entry-title'\n",
    "title_tag = soup.find('h1', {'id': 'post-title entry-title'})\n",
    "\n",
    "\n",
    "# Author\n",
    "all_author_tags = soup.find_all('a')\n",
    "\n",
    "# Filter the tags to find the author's link\n",
    "author_names = []\n",
    "for tag in all_author_tags:\n",
    "    # Check if the href contains 'author'\n",
    "    if 'author' in tag.get('href', ''):\n",
    "        # Append the author's name to the list, stripping any extra whitespace\n",
    "        author_names.append(tag.get_text(strip=True))\n",
    "\n",
    "\n",
    "author_name = author_names[0] # Author name is the first <a> tag \n",
    "\n",
    "\n",
    "# Date\n",
    "time_tag = soup.find('time', class_='date')\n",
    "\n",
    "# If the time tag is found, extract the datetime attribute and/or the text content\n",
    "if time_tag:\n",
    "    date_time = time_tag.get('datetime')  # Get the datetime attribute\n",
    "    display_date = time_tag.get_text(strip=True)  # Get the displayed date\n",
    "    print(f\"Datetime: {date_time}\")\n",
    "    print(f\"Displayed Date: {display_date}\")\n",
    "else:\n",
    "    print(\"Date not found\")\n",
    "\n",
    "\n",
    "\n",
    "# Find the first <img> tag  \n",
    "img_tag = soup.find('img')\n",
    "\n",
    "img_link = img_tag['src']\n",
    "\n",
    "\n",
    "#  Get Caption \n",
    "caption_tag = soup.find('p', class_='caption')\n",
    "\n",
    "if caption_tag:\n",
    "    caption_text = caption_tag.get_text(strip=True)  # Get the caption text and strip whitespace\n",
    "    print(f\"Caption: {caption_text}\")\n",
    "else:\n",
    "    print(\"Caption not found\")\n",
    "\n",
    "\n",
    "# Get TEXT \n",
    "content_div = soup.find('div', class_='tds-content')\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "# If the content div is found, find all <p> tags within it\n",
    "if content_div:\n",
    "    paragraphs = content_div.find_all('p')  # Find all <p> tags\n",
    "    for tag in paragraphs:\n",
    "        text += tag.get_text()\n",
    "\n",
    "    # Extract and print the text from each <p> tag\n",
    "    #for i, p in enumerate(paragraphs, start=1):\n",
    "    #    print(f\"Paragraph {i}: {p.get_text(strip=True)}\")  # Print each paragraph with a number\n",
    "else:\n",
    "    print(\"Content div not found\")\n",
    "\n",
    "cleaned_text = re.sub(r'\\bpic\\.twitter\\S*', '', text)\n",
    "\n",
    "    \n",
    "    \n",
    "# Create a CSV file and write the data\n",
    "with open(f'{fname}.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Title', 'Author', 'Date', 'Image URL', 'Figcaption', 'Article Text'])\n",
    "    \n",
    "    # Write the article data\n",
    "    writer.writerow([title_tag.get_text(), author_name, display_date, img_link, caption_text, cleaned_text])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c8fed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why Did Harris Lose the Election? Democratic Strategist Breaks It Down\n",
      "Donald Trump rode his way to a comeback with a quintessential law-and-order campaign: He exaggerated crime trends, talked about American cities as though they were war zones, and directed his ire at migrants, flooding the Republican National Convention with “Mass Deportation Now!” signs.It’s hard to know exactly how Trump’s harsh immigration policies will play out, but his campaign press secretary said that his new administration will start its mass deportation operation on day one.Trump won’t take office until January, but if you’re wondering whether he’ll make good on his promise to arrest, detain, and deport migrants, here’s one chart that paints a grim picture of what might be ahead:GEO Group, a private prison corporation that has had billions of dollars worth of contracts with the US Immigration Customs and Enforcement (ICE), saw a steep rise in its stock price on Wednesday — about 38 percent by the afternoon — after Trump was declared the winner of the presidential race. While that doesn’t translate to policy, it does show that the markets are betting that the private prison industry will likely flourish under the next Trump administration.This kind of market behavior is to be expected, since investors often overreact to news events. But even if today’s movement is a temporary price spike, that doesn’t change the fact that the company itself has been betting on a Trump presidency to make a fortune. GEO Group’s spending on its lobbying efforts peaked during the Trump administration, and slowed down after President Joe Biden announced that he would phase out the Department of Justice’s (DOJ) contracts with private prisons. The company had also donated to Trump’s inauguration in 2017 and even hosted one of its annual events at one of his properties.This time around, GEO Group’s PAC maxed out on its contributions to Trump’s campaign and gave a pro-Trump super PAC a $500,000 contribution.The reason is obvious: The company views a second Trump term as a boon to its business, which relies on government contracts. And since Trump is planning a mass deportation campaign, that would likely mean the federal government will arrest and detain more migrants. In a recent earnings call with its investors, GEO Group executives suggested that if Trump won, his policies could bolster the amount of money being sent to its detention facilities that have contracts with ICE.While Trump liked to tout his bipartisan criminal justice reform bill, which was enacted in 2018, signs like today’s market response to his victory show that private, for-profit prisons are expecting to see big returns when he returns to the White House. That’s an alarming reality because conditions in private prisons, including ones run by GEO Group, tend to be worse than in federal facilities, as a 2016 DOJ report pointed out. The problem was so bad that in 2016, the Obama administration announced it would phase out contracts with private prisons after the DOJ determined that they were less safe for inmates than federally run facilities. (Trump swiftly reversed that policy.)The markets don’t always predict policy. But when it comes to where investors are putting their money and who private prison companies like GEO Group supported for president, the bet they are making is clear: A second Trump term is lucrative if you’re in the business of incarcerating people. It’s a pretty good bet.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To CSV\n",
    "\n",
    "# Scripps\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "fname = 'Scripps'\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# Set a User-Agent header (optional, but often useful)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'\n",
    "}\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.scrippsnews.com/politics/america-votes/harris-calls-trump-to-concede-2024-election-source-says'\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')  # Use .content to pass the HTML content\n",
    "\n",
    "# Get title\n",
    "heading = soup.find('h1').get_text()\n",
    "print(heading)\n",
    "\n",
    "\n",
    "## Text\n",
    "main_content = soup.find('div', class_='Page-body ArticlePage-articleBody')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "if not article_body:\n",
    "    paragraphs = main_content.find_all('p')\n",
    "    for para in paragraphs:\n",
    "        article_text += para.get_text().strip() \n",
    "else:\n",
    "    for div in article_body:\n",
    "        paragraphs = div.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            article_text += paragraph.get_text().strip()\n",
    "            \n",
    "print(article_text)\n",
    "\n",
    "# Create a CSV file and write the data\n",
    "with open(f'{fname}.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['title', 'text'])\n",
    "    \n",
    "    # Write the article data\n",
    "    writer.writerow([heading,article_text])\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75e4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
